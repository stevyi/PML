---
title: "Practical Machine Learning: Project Write-up"
output:
  html_document
    
---
## Executive Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, our goal will be to use data from accelerometers on the belt, forearm, arm and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

### Preparing the environment
Firstly, load all libraries to be used throughout the project.

```{r, warning=F, message=F}
library(caret)
library(corrplot)
library(rpart)
library(rpart.plot)
library(kernlab)
library(knitr)
library(randomForest)
library(plyr)
```
```{r setoptions, echo = F}
opts_chunk$set(cache = F)
```

### Downloading and preprocessing the data
At first, 2 data files in ".csv" format in which contatining the training and testing data were downloaded into the specified working directory. 

```{r, eval = FALSE}
# Check the existing of a data folder, create one if otherwise
if (!file.exists("./data")) {dir.create("./data")}

# Save the URL file and destination file
trainingurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainingraw <- "./data/pml-training.csv"
testingurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testingraw <- "./data/pml-testing.csv"

# Download the training and testing URL files
download.file(trainingurl, destfile = trainingraw)
download.file(testingurl, destfile = testingraw)
```

The training and testing data were loaded into R

```{r}
# Read the training data from csv file
trainingData <- read.csv("./data/pml-training.csv", na.strings= c("NA",""," "))
dim(trainingData)
# Read the testing data from csv file
testingData <- read.csv("./data/pml-testing.csv", na.strings= c("NA",""," "))
dim(testingData)
```

### Data exploration and cleaning
There were a lot of NA values in the raw data which could be noises for the modeling in which required to be removed as part of data cleaning.

```{r}
# Clean up the data by removing columns with NAs
trainingRawData <- apply(trainingData, 2, function(x) {sum(is.na(x))})
trainingCleanedData <- trainingData[,which(trainingRawData == 0)]

# Remove and clean up the training data
trainingCleanedData <- trainingCleanedData[8:length(trainingCleanedData)]

# Apply the same treatment process to the final testing data
testingRawData <- apply(testingData, 2, function(x) {sum(is.na(x))})
testingCleanedData <- testingData[,which(testingRawData == 0)]

# Remove and clean up the testing data
testingCleanedData <- testingCleanedData[8:length(testingCleanedData)]
```

### Data modeling
The test data set was split up into training and cross validation sets in a 70:30 ratio.

```{r}
# split the cleaned testing data into training and cross validation
inTrainingData <- createDataPartition(y = trainingCleanedData$classe, p = 0.7, list = FALSE)
trainingData <- trainingCleanedData[inTrainingData, ]
XValidation <- trainingCleanedData[-inTrainingData, ]
```

A random forest model was selected to predict the classification as it is the best method for balancing error in class population unbalanced data sets. A correllation plot was produced to observe variables and their relationships with each other.

```{r, fig.height = 9, fig.width = 9}
# Plot a decision tree
treeModel <- rpart(classe ~ ., data=trainingData, method="class")
prp(treeModel)
```

In the decision tree plot, you can visulise the correlations and predictions among classification with all of the training data.

```{r, fig.height = 9, fig.width = 9}
# Plot a correlation matrix
correlateMatrix <- cor(trainingData[, -length(trainingData)])
corrplot(correlateMatrix, order = "FPC", method = "square", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
```

In the correlation matrix plot, the dark red and blue colours indicate a highly negative and positive relationship respectively between the variables. It is believed to be the best fit for the highly correlated predictions with all of the training data included in the model.

```{r}
# Use the best fit model to predict the classe
model <- randomForest(classe ~ ., data = trainingData)
model
```

The random forest model produced a very small OOB error rate of 0.56% for the training data which was deemed to be suitable for the evaluation on the testing data.

### Cross-validation
The model was used to classify the remaining 30% of data. The results were placed in a confusion matrix and statistics along with the actual classifications in order to determine the accuracy of the model.

```{r}
# Cross-validation the model using the remaining 30% of data
confusionMatrix(XValidation$classe, predict(model,XValidation))
```

This model yielded a 99.3% prediction accuracy and proved to be very robust and adequete to predict new data.

### Predictions on testing data
A separate data set was loaded into R and cleaned in the same treatment as previously perfromed was used to perform predictitive model with classifications on the testing data for 20 different test cases as required by the project.

```{r}
# Prediction model to predict 20 different test cases and results
predictTest <- predict(model, testingCleanedData)
predictTest
```

### Generate text files for submission
The following function create the text files for answers to the prediction assignment submission

```{r}
# Create 20 text files for the assignment submission
PML_Submission = function(x){
  Num=length(x)
  for (index in 1:Num) {
    filename = paste0("./results/submission_",index,".txt")
    write.table(x[index],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
PML_Submission(predictTest)
```

## Conclusions
In summary, it is possible to predict accurately how well a person is preforming an excercise using a relatively simple random forest model. 